{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import lancedb\n",
    "import pyarrow as pa\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image path for dataset: .\\keyframes\\*\\*\n",
      "Number of images:  106589\n",
      "Number of batches:  417\n"
     ]
    }
   ],
   "source": [
    "# Dataset and dataloader\n",
    "class KeyframesDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        print(\"Image path for dataset: .\\\\keyframes\\\\*\\\\*\")\n",
    "        self.images_paths = glob.glob(\".\\\\keyframes\\\\*\\\\*\")\n",
    "\n",
    "        map_keyframes_paths = glob.glob('.\\\\map-keyframes\\\\*.csv')\n",
    "        self.map_keyframes_dfs = {}\n",
    "        for path in map_keyframes_paths:\n",
    "            self.map_keyframes_dfs[path.rsplit('\\\\', 1)[1][:-4]] = pd.read_csv(path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.images_paths[idx])\n",
    "        video_name = self.images_paths[idx].rsplit('\\\\', 2)[1]\n",
    "        image_name = self.images_paths[idx].rsplit('\\\\', 1)[1]\n",
    "        frame_idx = self.map_keyframes_dfs[video_name].at[int(image_name[:-4])-1, 'frame_idx']\n",
    "        return image, video_name, image_name, frame_idx, self.images_paths[idx]\n",
    "\n",
    "dataset = KeyframesDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "print(\"Number of images: \", len(dataset))\n",
    "print(\"Number of batches: \", len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\minhh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Table patch14v2 already exists!\n"
     ]
    }
   ],
   "source": [
    "# Create lancedb instance\n",
    "lancedb_instance = lancedb.connect(\"database.lance\")\n",
    "TABLE_NAME = \"patch14v2\"\n",
    "if TABLE_NAME in lancedb_instance.table_names():\n",
    "    database = lancedb_instance[TABLE_NAME]\n",
    "    print(f\"Warning: Table {TABLE_NAME} already exists!\")\n",
    "else:\n",
    "    schema = pa.schema([\n",
    "        pa.field(\"embedding\", pa.list_(pa.float32(), 768)),\n",
    "        pa.field(\"video_name\", pa.string()),\n",
    "        pa.field(\"image_name\", pa.string()),\n",
    "        pa.field(\"frame_idx\", pa.int32()),\n",
    "        pa.field(\"path\", pa.string()),\n",
    "    ])\n",
    "    lancedb_instance.create_table(TABLE_NAME, schema=schema)\n",
    "    database = lancedb_instance[TABLE_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcf0f98fe3b42ffbca15481d8b3a7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\minhh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:480: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for batch 20/417\n",
      "Saved embeddings for batch 40/417\n",
      "Saved embeddings for batch 60/417\n",
      "Saved embeddings for batch 80/417\n",
      "Saved embeddings for batch 100/417\n",
      "Saved embeddings for batch 120/417\n",
      "Saved embeddings for batch 140/417\n",
      "Saved embeddings for batch 160/417\n",
      "Saved embeddings for batch 180/417\n",
      "Saved embeddings for batch 200/417\n",
      "Saved embeddings for batch 220/417\n",
      "Saved embeddings for batch 240/417\n",
      "Saved embeddings for batch 260/417\n",
      "Saved embeddings for batch 280/417\n",
      "Saved embeddings for batch 300/417\n",
      "Saved embeddings for batch 320/417\n",
      "Saved embeddings for batch 340/417\n",
      "Saved embeddings for batch 360/417\n",
      "Saved embeddings for batch 380/417\n",
      "Saved embeddings for batch 400/417\n",
      "Saved embeddings for batch 417/417\n"
     ]
    }
   ],
   "source": [
    "LEN_DATALOADER = len(dataloader)\n",
    "SAVE_EVERY = int(0.05 * LEN_DATALOADER)\n",
    "\n",
    "df = pd.DataFrame(columns=['embedding', 'video_name', 'image_name', 'frame_idx', 'path'])\n",
    "\n",
    "for i, (images, video_names, image_names, frame_idxs, paths) in enumerate(tqdm(dataloader)):\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        embeddings = model.get_image_features(**inputs).cpu().squeeze().numpy()\n",
    "    data = {\n",
    "        'embedding': [],\n",
    "        'video_name': [],\n",
    "        'image_name': [],\n",
    "        'frame_idx': [],\n",
    "        'path': []\n",
    "    }\n",
    "    for embedding, video_name, image_name, frame_idx, path in zip(embeddings, video_names, image_names, frame_idxs, paths):\n",
    "        data['embedding'].append(embedding)\n",
    "        data['video_name'].append(video_name)\n",
    "        data['image_name'].append(image_name)\n",
    "        data['frame_idx'].append(int(frame_idx))\n",
    "        data['path'].append(path)\n",
    "    df = pd.concat([df, pd.DataFrame(data)], ignore_index=True)\n",
    "    if (i + 1) % SAVE_EVERY == 0 or i + 1 == LEN_DATALOADER:\n",
    "        lancedb_instance[TABLE_NAME].add(df)\n",
    "        df = pd.DataFrame(columns=['embedding', 'video_name', 'image_name', 'frame_idx', 'path'])\n",
    "        print(f\"Saved embeddings for batch {i+1}/{LEN_DATALOADER}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
